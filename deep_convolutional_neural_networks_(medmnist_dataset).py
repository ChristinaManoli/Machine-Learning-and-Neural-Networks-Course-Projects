# -*- coding: utf-8 -*-
"""Deep Convolutional Neural Networks (MedMNIST Dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16V1VXQ6l4U6HDIw6_-XkxZAmWqTpRqDt

# Εργασία 2: Αυτόματη διάγνωση σε ιατρικές εικόνες

Πέρα από τα εργαστήρια 2a και 2b, μπορείτε να βρείτε έτοιμο σχετικό κώδικα και εδώ:

https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb

Εγκαθιστούμε τη βιβλιοθήκη `medmnist` στο virtual machine μας. (Χρειάζεται μόνο την πρώτη φορά κάθε session).
"""

!pip install medmnist

"""## Εισαγωγή αναγκαίων βιβλιοθηκών και πακέτων
Μπορείτε να επεκτείνετε αυτόν τον κώδικα με ό,τι χρειάζεστε
"""

from tqdm import tqdm

import torch
import torchvision.transforms as transforms
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
from torch.optim import lr_scheduler

import medmnist
from medmnist import INFO
from sklearn.metrics import f1_score, accuracy_score
import numpy as np

print("Pytorch version:", torch.__version__)
print("GPU available:", torch.cuda.is_available())
print(f"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}")

"""## Transforms
Θα χρησιμοποιήσουμε τις ίδιες προεπεξεργασίες για όλα τα δεδομένα μας ώστε τα διαφορετικά μοντέλα να είναι συμβατά μεταξύ τους.

Αυτό στηρίζεται στο ότι οι εικόνες των datasets που θα χρησιμοποιήσουμε θα έχουν ίσες διαστάσεις μεταξύ τους (28$\times$28$\times$3)
"""

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ]),
    'test': transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ]),
}

"""## Εισαγωγή του Colon Pathology dataset """

# Το αντικείμενο medmnist.INFO μανατζάρει όλα τα datasets του medmnist. Με το κλειδί 'pathmnist' (το βρήκαμε στο documentation) επιλέγουμε το Colon Pathology Dataset
path_info = INFO['pathmnist']
path_task = path_info['task']
path_channels = path_info['n_channels']
path_classes = len(path_info['label'])

print ("Οι εικόνες του dataset έχoυν", path_channels, "κανάλια. Ο τύπος προβλήματος είναι", path_task, "ταξινόμηση με", path_classes, "κλάσεις.")

# Το πεδίο 'python_class' του info επιστρέφει μια κλάση της python που λειτουργεί ως pytorch dataset, για να την περάσουμε στο dataloader
Path_DataClass = getattr(medmnist, path_info['python_class'])

# Φόρτωση (και κατέβασμα) δεδομένων
path_train_dataset = Path_DataClass(split='train', transform=data_transforms['train'], download=True)
path_val_dataset = Path_DataClass(split='val', transform=data_transforms['test'], download=True)
path_test_dataset = Path_DataClass(split='test', transform=data_transforms['test'], download=True)

#Το .info είναι ένα dictionary με επεξηγηματικές πληροφορίες για το dataset
print("---------------------------------------------------------------------")
print("Διαθέσιμες πληροφορίες για το dataset:")
print(path_train_dataset.info.keys())
print("Περιγραφή dataset:")
print(path_train_dataset.info['description'])
print("Ερμηνεία labels:")
print(path_train_dataset.info['label'])

BATCH_SIZE = 128

# encapsulate data into dataloader form
path_train_loader = data.DataLoader(dataset=path_train_dataset, batch_size=BATCH_SIZE, shuffle=True)
path_val_loader = data.DataLoader(dataset=path_val_dataset, batch_size=2*BATCH_SIZE, shuffle=False)
path_test_loader = data.DataLoader(dataset=path_val_dataset, batch_size=2*BATCH_SIZE, shuffle=False)

print("Δείγμα εικόνων του Colon Pathology dataset:")
path_train_dataset.montage(length=4)

"""## Δημιουργία μοντέλου

Ο ενδεικτικός κώδικας του MedMNIST προβλέπει τόσο ο αριθμός των καναλιών των εικόνων όσο και ο αριθμός των κλάσεων να εισάγονται ως παράμετροι. Αυτή η ευελιξία δεν μας είναι απαραίτητη καθώς εμείς θα δημιουργήσουμε ένα δίκτυο που θα δέχεται εικόνες 3 καναλιών και θα επιστρέφει 9 κλάσεις, συγκεκριμένα για το PathMNIST. Οποιαδήποτε αλλαγή κάνουμε (π.χ. στον αριθμό των κλάσεων/μονάδων εξόδου) θα γίνει αφού δημιουργήσουμε ένα στιγμιότυπο του ίδιου δικτύου.

Παρόλα αυτά, αν προτιμάτε να τα αφήσετε ως παραμέτρους, μπορείτε.
"""

import torch.nn as nn


class Net(nn.Module):
    print("Δομή μοντέλου εδώ")
    def __init__(self, num_classes):
        super(Net, self).__init__()

        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU())

        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU())

        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, padding=2),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        
        self.layer4 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer5 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))

        self.fc = nn.Sequential(
            nn.Linear(64 * 3 * 3, 1024),
            nn.ReLU())
        
        self.outputL = nn.Sequential(
            nn.Linear(1024, num_classes))

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        x = self.outputL(x)
        return x



net = Net(num_classes=9)

"""##4. Αξιοποίηση CUDA

"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)

#Φορτώνουμε το δίκτυο στη GPU
net.to(device)
#Εναλλακτικά net.cuda()

"""##5. Παραμετροποίηση"""

print("Λοιπός κώδικας παραμετροποίησης, training, testing κλπ εδώ")

import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)
scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)

criterion = nn.CrossEntropyLoss()

epochs = 9

"""#**6. Training και Test Loop**

## Training Loop
"""

def train_loop(dataloader, model, criterion, optimizer):
  running_loss = 0.0

  for inputs, targets in tqdm(dataloader):
    # zero the parameter gradients
    optimizer.zero_grad()
  
    # forward + backward + optimize

    #Φορτώνουμε κάθε batch στη GPU
    images = inputs.to(device)
    labels = targets.to(device)
    outputs = model(images)

    t = labels.squeeze().long()
    loss = criterion(outputs, t)
    loss.backward()
    
    #Ο optimizer θα κάνει ένα step για κάθε batch 
    optimizer.step()

    running_loss += loss.item()

  print(f"\nTraining Loss: {running_loss:.3f}")
  print('Finished Training\n')

"""##Testing Loop"""

def test_loop(dataloader, model):
  accuracy = 0.0
  Thetargets =[]
  Theoutputs = []
  with torch.no_grad():
    for inputs, targets in tqdm(dataloader):

      #Φορτώνουμε κάθε batch στη GPU
      images = inputs.to(device)
      labels = targets.to(device)

      outputs = model(images)
      _, preds = torch.max(outputs, 1)

      #Keep all targets and predictions of an epoch in a list in order to use them later in accuracy score
      Thetargets.append(labels.cpu().flatten().tolist())
      
      Theoutputs.append(preds.cpu().flatten().tolist())
      

  #print("Targets:", Thetargets)
  #print("Targets:", np.hstack(Thetargets))
  #print("predictions:" , Theoutputs)
  
  #Χρησιμοποιούμε την μέθοδο accuracy_score από τη βιβλιοθήκη sklearn, η οποία δέχεται όλα τα targets και όλα τα outputs ενός dataset και επιστρέφει το accuracy
  accuracy = accuracy_score(np.hstack(Thetargets), np.hstack(Theoutputs))
  print(f"\nAccuracy: { accuracy:.3f}\n")

"""######Στην περίπτωση πολύ άνισων κλάσεων, ποιά θα ήταν μια καταλληλότερη μετρική;



> Απάντηση: Στην περίπτωση άνισων κλάσεων θα μπορούσαμε να χρησιμοποιήσουμε είτε precision και recall είτε τον συνδυασμό των δύο παραπάνω, F-measure.

#**7. Εκπαίδευση στο PathMNIST dataset**
"""

for epoch in range(epochs):
  print(f"Epoch {epoch+1}\n-------------------------------")
  print(f"Training:")
  train_loop(path_train_loader, net, criterion, optimizer)
  
  #O scheduler κάνει ένα step στο τέλος κάθε Epoch 
  scheduler.step()
  
  print(f"Test Loop σε training dataset:")
  test_loop(path_train_loader, net)
  
  print(f"Test Loop σε  validation dataset:")
  test_loop(path_val_loader, net)

print("Done!\n-------------------------------")

print(f"Test Loop σε test dataset:")
test_loop(path_test_loader, net)

"""##**Σχολιασμός**
*Φαίνεται να μαθαίνει το δίκτυο; Βλέπετε συμπτώματα overfitting ή όχι; Από πού θα το συμπεράνετε;*


> Απάντηση: Μπορούμε να παρατηρήσουμε ότι το δίκτυο μας μαθαίνει καθώς απο το 2ο μολις epoch η διαφορά τοσο στο training Loss όσο και στο Accuracy ειναι εμφανής. Επιπλέον, μπορούμε να παρατηρήσουμε την σταθερή και συνεχή αύξηση του accuracy ανα τα epochs φτάνοντας μέχρι και το **0.975** για το **training set** και **0.969** για το **validation set**. Σύμφωνα με τα παραπάνω και γνωρίζοντας ότι το accuracy για το **test set** είναι **0.969** μπορούμε να συμπεράνουμε ότι το δίκτυο μας δεν παρουσιάζει συμπτώματα overfitting.

#**8. Εκπαίδευση με Transfer Learning στο BloodMNIST dataset**

### Εισαγωγή του Blood Cell Microscope dataset
"""

data_flag = 'bloodmnist'
download = True

info = INFO[data_flag]
task = info['task']
print (task)
blood_channels = info['n_channels'] # αν τα κανάλια στα inputs είναι διαφορετικά, το transfer learning δεν έχει νόημα γιατί τα δεδομένα δε θα έχουν καμία ομοιότητα
blood_classes = len(info['label'])

Blood_Dataclass = getattr(medmnist, info['python_class'])

# load the data
blood_train_dataset = Blood_Dataclass(split='train', transform=data_transforms['train'], download=download)
blood_val_dataset = Blood_Dataclass(split='val', transform=data_transforms['test'], download=download)
blood_test_dataset = Blood_Dataclass(split='test', transform=data_transforms['test'], download=download)
print(blood_test_dataset.info)

# encapsulate data into dataloader form
blood_train_loader = data.DataLoader(dataset=blood_train_dataset, batch_size=BATCH_SIZE, shuffle=True)
blood_val_loader = data.DataLoader(dataset=blood_val_dataset, batch_size=2*BATCH_SIZE, shuffle=False)
blood_test_loader = data.DataLoader(dataset=blood_test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)

print("Δείγμα εικόνων του Blood Cell Microscope dataset:")
blood_train_dataset.montage(length=4)

import copy

net2 = Net(num_classes=9)

model_wts = copy.deepcopy(net.state_dict())
net2.load_state_dict(model_wts)

net2.outputL = nn.Linear(1024, 8)
net2.to(device)

optimizer2 = optim.SGD(net2.parameters(), lr=0.005, momentum=0.9)
scheduler2 = lr_scheduler.StepLR(optimizer2, step_size=2, gamma=0.5)

for epoch in range(epochs):
  print(f"Epoch {epoch+1}\n-------------------------------")
  train_loop(blood_train_loader, net2, criterion, optimizer2)
  #O scheduler κάνει ένα step στο τέλος κάθε Epoch 
  scheduler2.step()
  
  print(f"Test Loop σε training dataset: \n")
  test_loop(blood_train_loader, net2)
  print(f"Test Loop σε  validation dataset: \n")
  test_loop(blood_val_loader, net2)

print("Done!")
print(f"Test Loop σε test dataset: \n")
test_loop(blood_test_loader, net2)

"""#**9. Εκπαίδευση χωρίς Transfer Learning στο BloodMNIST dataset**"""

net3 = Net(num_classes=8)


optimizer3 = optim.SGD(net3.parameters(), lr=0.005, momentum=0.9)
scheduler3 = lr_scheduler.StepLR(optimizer3, step_size=2, gamma=0.5)
net3.to(device)

for epoch in range(epochs):
  print(f"Epoch {epoch+1}\n-------------------------------")
  train_loop(blood_train_loader, net3, criterion, optimizer3)
  #O scheduler κάνει ένα step στο τέλος κάθε Epoch 
  scheduler3.step()
  
  print(f"Test Loop σε training dataset: \n")
  test_loop(blood_train_loader, net3)
  print(f"Test Loop σε  validation dataset: \n")
  test_loop(blood_val_loader, net3)

print("Done!")
print(f"Test Loop σε test dataset: \n")
test_loop(blood_test_loader, net3)

"""##**Σχολιασμός:**
*Φαίνεται να μαθαίνει το δίκτυο; Φαίνεται να πάσχει 
από overfitting; Συμπεραίνετε ότι το Transfer Learning συνεισέφερε κάτι μεταξύ των δύο datasets*


Αρχικά, όσον αφορά το **Transfer Learning**, παρατηρήσαμε ότι η μεταφορά των βαρών από το προηγούμενο εκπαιδευμένο δίκτυο επέφερε εμφανή βελτίωση στην εκπαίδευση του νέου δικτύου καθώς απο το 2ο epoch η διαφορά που υπάρχει στις μετρικές είναι εμφανής σε σχέση με πριν.

Όσον αφορά, το παραπάνω δίκτυο (**net3**) παρατηρούμε ότι συνεχώς μαθαίνει καθώς υπάρχει **συνεχής βελτίωση** ανα τα epochs τόσο στο Training Loss όσο και στο Accuracy.

Επιπλέον, το δίκτυο μας **δεν φαίνεται να πάσχει από overfitting** καθώς το **Accuracy** του test set είναι στο **0.947** πράγμα που σημαίνει ότι το δικτυό μας καταφέρνει να γενικεύει και να επιστρέφει σωστές κλάσεις για τα άγνωστα αντικέιμενα που βλέπει για πρώτη φορά. 

"""